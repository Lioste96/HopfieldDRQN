# HopfieldDRQN
Over the past years Reinforcement learning has experienced substantial progress and has even achieved to surpass human capability in some tasks like Dota 2, Chess and Go and has brought Deep Reinforcement Learning back into the spotlight. However, reinforcement learning agents still face substantial difficulty to reach human capability in RL frameworks that are encompassed by partial observability, a sparse reward system and an insufficient number of samples. A strategy to overcome some of these problems and has received a great amount of attention over the years by the machine learning society incorporates the usage of memory into deep reinforcement learning architectures. Most applications for continuous states and partially observable environments use different types of recurrent neural networks such as LSTMs and GRUs as memory to store and retrieve past experiences in a model-free framework, an example of such an architecture is Deep Recurrent Q-Network. Nevertheless, machine learning scientists have been searching for substitutes. An alternative to original recurrent neural networks is Modern Continuous Hopfield Networks introduced by Ramsauer et al., in 2020, which can be utilised in reinforcement learning algorithms to store and retrieve agent experiences due to their continuous and differentiable state. 

This project tests whether Modern Continuous Hopfield Networks can be efficiently embedded in deep reinforcement learning architectures and if it is possible to provide similar results compared to other memory methodologies like LSTM and GRU. The main algorithm used is a Deep Recurrent Q-Network, in which the LSTM layer is replaced with Modern Continuous Hopfield Layers and other types of recurrent layers like GRU to compare the efficiency of the algorithm in a variety of different environments. The architecture of the Deep Recurrent Q-Network is even further adjusted with Convolutional Neural Networks or Fully Connected Layers depending on the dimensionality of the environmentâ€™s observational space.

Keywords: Modern Continuous Hopfield Networks, Deep Recurrent Q Networks, Recurrent Neural Networks, Long Short-Term Memory, Gated Recurrent Unit
